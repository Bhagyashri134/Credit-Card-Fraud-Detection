# -*- coding: utf-8 -*-
"""AI Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JlfLzCaq47_xm0G_51otU3cG_6JTBmV0

## **Phase  1**

## **Load the Dataset**
"""

import pandas as pd

## Load the dataset
df = pd.read_csv("/content/creditcard.csv")

## Display the first 5 Rows
print(df.head())

## Check dataset info
print(df.info())

## Check the missing Values
print(df.isnull().sum())

## Basic statistics of the dataset
print(df.describe())     ##  Generates summary statistics for numerical columns.

"""

# **Explore the Dataset**
The dataset contains 31 columns:

Time: Time elapsed since the first transaction.

V1-V28: Anonymized features (result of PCA transformation).

Amount: Transaction amount.

Class: Target variable (0 for non-fraudulent, 1 for fraudulent)."""

## count of fraudulent vs non- fraudulent transaction

fraud_counts = df['Class'].value_counts()
print(fraud_counts)

#visualize the destribution
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))
sns.countplot(x= 'Class', data=df)
plt.title('Fraudulent vs Non-Fraudulent Transactions')
plt.xlabel('Class (0 = Non-Fraud, 1 = Fraud)')
plt.ylabel('Count')
plt.show()

"""## **Phase 2:** Data Collection & Preprocessing (Week 2)

Feature Engineering
"""

from sklearn.preprocessing import StandardScaler

# Normalize 'Amount' and 'Time'
scaler = StandardScaler()
df['Amount'] = scaler.fit_transform(df[['Amount']])
df['Time'] = scaler.fit_transform(df[['Time']])

# Display the first 5 rows after normalization
print(df.head())

"""Handle Class Imbalance"""

# Import the library
from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE

# Sample Dataset
X, y = make_classification(n_samples=1000, n_features=5, weights=[0.1, 0.9], flip_y=0, random_state=42)

# Convert to DataFrame for visualization
df = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'])
df['Target'] = y

# Apply SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Check the new class distribution
print(pd.Series(y_resampled).value_counts())

"""Save the Preprocessed Data"""

# Convert resampled features and target to DataFrame
X_resampled_df = pd.DataFrame(X_resampled, columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'])
y_resampled_df = pd.DataFrame(y_resampled, columns=['Target'])

# Concatenate both DataFrames
df_resampled = pd.concat([X_resampled_df, y_resampled_df], axis=1)

# Save to CSV
df_resampled.to_csv('creditcard_cleaned.csv', index=False)

"""# **Phase 3: Model Selection & Training (Week 3)**

Step 1: Split the Data
"""

from sklearn.model_selection import train_test_split

# Split the data (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

print(f"Training set: {X_train.shape}, {y_train.shape}")
print(f"Testing set: {X_test.shape}, {y_test.shape}")

"""Step 2: Train Multiple Models"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'XGBoost': XGBClassifier()
}

# Train and evaluate models
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Evaluate performance
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results[name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    }

# Display results
for name, metrics in results.items():
    print(f"{name}: {metrics}")

"""Step 3: Choose the Best Model"""

# Convert results to a DataFrame for better visualization
results_df = pd.DataFrame(results).T
print(results_df)

"""# **Phase 4: Model Optimization & Deployment (Week 4)**

**Step** 1: Hyperparameter Tuning
"""

from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=3, scoring='f1')

# Fit the model
grid_search.fit(X_train, y_train)

# Best parameters and score
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best F1 Score: {grid_search.best_score_}")

"""Step 2: Deploy the Model Using Flask"""

# Run this code first to create the model file
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
import pickle

# Use the same parameters as in your original code
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=3, scoring='f1')

# Fit the model (using your training data)
grid_search.fit(X_train, y_train)

# Save the trained model
with open('fraud_model.pkl', 'wb') as f:
    pickle.dump(grid_search.best_estimator_, f)

"""Step 3: Build a UI Using Streamlit"""